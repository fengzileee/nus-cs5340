{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ff0155",
   "metadata": {},
   "source": [
    "# Tests and sanity checks of HMM implementations\n",
    "\n",
    "For basic functionality tests of our HMM implementations, we use a simple Markov chain setup based on a coin-flipping task.\n",
    "\n",
    "Consider a coin-flipping task where there are 2 states: 1) we are holding an unbiased coin and 2) we are holding a biased coin. Between coin flips, there is a probability p that we switch coins. This gives a Markov chain with a 2x2 transition matrix. The unbiased coin state has index 0 and the biased coin state has index 1.\n",
    "    \n",
    "There are two possible observations/emissions from each coin flip. We can either observe: 1) heads or 2) tails. The probability of observing heads/tails from the unbiased coin state is 0.5, and the probability from the biased coin state is q/1-q, for some probability q. The heads emission has index 0 and the tails emission has index 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "addb039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49166a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transition_matrix(p):\n",
    "    return np.array([[p, 1-p], [1-p, p]])\n",
    "\n",
    "def make_emission_matrix(q):\n",
    "    return np.array([[0.5, 0.5], [q, 1-q]])\n",
    "\n",
    "def draw_sequence(A, B, prior, T):\n",
    "    num_states, num_obs = B.shape\n",
    "    state = np.random.choice(num_states, 1, p=prior).item()\n",
    "    emissions = [np.random.choice(num_obs, 1, p=B[state].flatten()).item()]\n",
    "    for t in range(1, T):\n",
    "        state = np.random.choice(num_states, 1, p=A[state].flatten()).item()\n",
    "        emissions.append(np.random.choice(num_obs, 1, p=B[state].flatten()).item())\n",
    "    return emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd8802",
   "metadata": {},
   "source": [
    "## Multinomial HMM based on 1st order Markov chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb1e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertainty_motion_prediction.predictor import HMMMultinomialFirstOrder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6205abb0",
   "metadata": {},
   "source": [
    "We test the forward/backward algorithms used for estimating the likelihood of a given sequence of emissions/observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d901f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "q = 0.5\n",
    "transition_matrix = make_transition_matrix(p)\n",
    "emission_matrix = make_emission_matrix(q)\n",
    "prior = np.array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0424ba74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0625 0.06250000000000003\n"
     ]
    }
   ],
   "source": [
    "hmm1 = HMMMultinomialFirstOrder(2, 2, verbose=True)\n",
    "hmm1.initialise_parameters(transition_matrix, prior, emission_matrix)\n",
    "test_seq1 = np.array([0, 0, 1, 0])\n",
    "prob1 = hmm1.get_sequence_likelihood_backward(test_seq1)\n",
    "prob2 = hmm1.get_sequence_likelihood(test_seq1)\n",
    "\n",
    "print(prob1, prob2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c954140",
   "metadata": {},
   "source": [
    "We test the decoding algorithms used to estimate the most likely sequence of hidden states that produced the sequence of emissions/observations in our data. The decoding algorithm implemented is the Viterbi algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80ade997",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "q = 0.999 # Use 0.999 instead of 1, because I have yet to handle the case of np.log(0) in log-sum-exp function.\n",
    "transition_matrix = make_transition_matrix(p)\n",
    "emission_matrix = make_emission_matrix(q)\n",
    "prior = np.array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caaf85a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1]\n",
      "0.06237506250000003\n"
     ]
    }
   ],
   "source": [
    "hmm2 = HMMMultinomialFirstOrder(2, 2, verbose=True)\n",
    "hmm2.initialise_parameters(transition_matrix, prior, emission_matrix)\n",
    "test_seq2 = np.array([1, 0, 0])\n",
    "path, prob = hmm2.decode(test_seq2)\n",
    "print(path)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b462d",
   "metadata": {},
   "source": [
    "Test the estimation of HMM model parameters from data. This uses the specialised form of the EM algorithm, the Baum-Welch algorithm. The algorithm can be prone to local minima, and may not converge well when all the parameters to be estimated are not well-initialised. You can test its robustness to initialisation but selectively initialising some of the parameters using the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d73edc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "q = 0.999 # Use 0.999 instead of 1, because I have yet to handle the case of np.log(0) in log-sum-exp function.\n",
    "transition_matrix = make_transition_matrix(p)\n",
    "emission_matrix = make_emission_matrix(q)\n",
    "prior = np.array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f45899fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 1 1 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 1]]\n",
      "Estimating HMM model parameters...\n",
      "Iter 1, log-likelihood loss: -7.024722607974975, delta: inf\n",
      "Iter 2, log-likelihood loss: -5.664936102659883, delta: 1.3597865053150917\n",
      "Iter 3, log-likelihood loss: -5.66131013548803, delta: 0.0036259671718532616\n",
      "Iter 4, log-likelihood loss: -5.658889304612112, delta: 0.0024208308759181563\n",
      "Iter 5, log-likelihood loss: -5.657255492287608, delta: 0.0016338123245036584\n",
      "Iter 6, log-likelihood loss: -5.656145098736711, delta: 0.0011103935508973706\n",
      "Iter 7, log-likelihood loss: -5.655387159901618, delta: 0.0007579388350924532\n",
      "Iter 8, log-likelihood loss: -5.65486849272569, delta: 0.0005186671759283001\n",
      "Iter 9, log-likelihood loss: -5.654513089781826, delta: 0.0003554029438639361\n",
      "Iter 10, log-likelihood loss: -5.654269421616268, delta: 0.00024366816555776438\n",
      "Iter 11, log-likelihood loss: -5.654102342983989, delta: 0.00016707863227960473\n",
      "Iter 12, log-likelihood loss: -5.653987797731999, delta: 0.00011454525198928422\n",
      "Iter 13, log-likelihood loss: -5.653909288519687, delta: 7.85092123125608e-05\n",
      "Iter 14, log-likelihood loss: -5.653855492412965, delta: 5.379610672218149e-05\n",
      "Iter 15, log-likelihood loss: -5.65381863681706, delta: 3.685559590493881e-05\n",
      "Iter 16, log-likelihood loss: -5.653793387648064, delta: 2.5249168995600257e-05\n",
      "Iter 17, log-likelihood loss: -5.653776086052482, delta: 1.730159558199773e-05\n",
      "Iter 18, log-likelihood loss: -5.6537642237406756, delta: 1.1862311806609682e-05\n",
      "Iter 19, log-likelihood loss: -5.653756082226236, delta: 8.141514439508057e-06\n",
      "Transition\n",
      "[[0.31238621 0.68761379]\n",
      " [0.17969314 0.82030686]]\n",
      "Emission\n",
      "[[0.75423715 0.24576285]\n",
      " [0.74447049 0.25552951]]\n",
      "Prior\n",
      "[0.83414374 0.16585626]\n",
      "Estimating HMM model parameters...\n",
      "Iter 1, log-likelihood loss: -5.653750484830988, delta: inf\n",
      "Iter 2, log-likelihood loss: -5.653746626305527, delta: 3.858525460920248e-06\n",
      "Loaded and Trained Transition\n",
      "[[0.31225263 0.68774737]\n",
      " [0.1797245  0.8202755 ]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([draw_sequence(transition_matrix, emission_matrix, prior, 10) for i in range(500)])\n",
    "print(data)\n",
    "\n",
    "# Don't give the correct values for any of the parameters. Use random initialisation.\n",
    "hmm3 = HMMMultinomialFirstOrder(2, 2, verbose=True)\n",
    "hmm3.estimate_parameters(data)\n",
    "\n",
    "print(\"Transition\")\n",
    "print(np.exp(hmm3._log_A))\n",
    "print(\"Emission\")\n",
    "print(np.exp(hmm3._log_B))\n",
    "print(\"Prior\")\n",
    "print(np.exp(hmm3._log_phi))\n",
    "\n",
    "hmm3.save_to_file(\"./hmm_sanity_check_param.pickle\")\n",
    "\n",
    "hmm3_load = HMMMultinomialFirstOrder(2, 2, verbose=True)\n",
    "hmm3_load.load_from_file(\"./hmm_sanity_check_param.pickle\")\n",
    "hmm3_load.estimate_parameters(data)\n",
    "print(\"Loaded and Trained Transition\")\n",
    "print(np.exp(hmm3_load._log_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eb2d9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 1 1 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 1]]\n",
      "Estimating HMM model parameters...\n",
      "Iter 1, log-likelihood loss: -7.3160232371448695, delta: inf\n",
      "Iter 2, log-likelihood loss: -5.653891440391395, delta: 1.6621317967534743\n",
      "Iter 3, log-likelihood loss: -5.653879230677302, delta: 1.220971409310323e-05\n",
      "Iter 4, log-likelihood loss: -5.653867394295645, delta: 1.1836381657026607e-05\n",
      "Iter 5, log-likelihood loss: -5.653855915128039, delta: 1.1479167605799034e-05\n",
      "Iter 6, log-likelihood loss: -5.653844777992499, delta: 1.113713554001805e-05\n",
      "Iter 7, log-likelihood loss: -5.653833968573029, delta: 1.0809419470625414e-05\n",
      "Iter 8, log-likelihood loss: -5.653823473355083, delta: 1.0495217945383217e-05\n",
      "Iter 9, log-likelihood loss: -5.65381327956661, delta: 1.0193788472889764e-05\n",
      "Iter 10, log-likelihood loss: -5.653803375123801, delta: 9.9044428090167e-06\n",
      "Transition\n",
      "[[0.45704322 0.54295678]\n",
      " [0.45960241 0.54039759]]\n",
      "Emission\n",
      "[[0.68387724 0.31612276]\n",
      " [0.80043714 0.19956286]]\n",
      "Prior\n",
      "[0.44145927 0.55854073]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([draw_sequence(transition_matrix, emission_matrix, prior, 10) for i in range(500)])\n",
    "print(data)\n",
    "\n",
    "# Initialise the transition and prior models using correct values, and try to estimate the emission probabilities.\n",
    "hmm3 = HMMMultinomialFirstOrder(2, 2, verbose=True)\n",
    "hmm3._initialise_transition_model(transition_matrix)\n",
    "hmm3._initialise_prior_distribution(prior)\n",
    "hmm3.estimate_parameters(data)\n",
    "\n",
    "print(\"Transition\")\n",
    "print(np.exp(hmm3._log_A))\n",
    "print(\"Emission\")\n",
    "print(np.exp(hmm3._log_B))\n",
    "print(\"Prior\")\n",
    "print(np.exp(hmm3._log_phi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15000bbf",
   "metadata": {},
   "source": [
    "## Multinomial HMM based on 2nd order Markov chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d54ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertainty_motion_prediction.predictor import HMMMultinomialSecondOrder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f04d251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transition_matrix(p):\n",
    "    return np.array([[[p, 1-p], [1-p, p]], [[p, 1-p], [1-p, p]]])\n",
    "\n",
    "def make_emission_matrix(q):\n",
    "    return np.array([[0.5, 0.5], [q, 1-q]])\n",
    "\n",
    "def draw_sequence(A, B, prior, T):\n",
    "    num_states, num_obs = B.shape\n",
    "    state = np.random.choice(num_states, 1, p=prior).item()\n",
    "    emissions = [np.random.choice(num_obs, 1, p=B[state].flatten()).item()]\n",
    "    last_state = np.random.choice(num_states, 1, p=prior).item() \n",
    "    for t in range(1, T):\n",
    "        state = np.random.choice(num_states, 1, p=A[last_state][state].flatten()).item()\n",
    "        emissions.append(np.random.choice(num_obs, 1, p=B[state].flatten()).item())\n",
    "        last_state = state \n",
    "    return emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "809c9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from uncertainty_motion_prediction.predictor import HMMBase\n",
    "\n",
    "# reference: https://journals.sagepub.com/doi/full/10.1177/1550147718772541\n",
    "\n",
    "class HMMMultinomialSecondOrder(HMMBase):\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 obs_dim,\n",
    "                 tol=1e-5,\n",
    "                 max_iters=1,\n",
    "                 verbose=False):\n",
    "\n",
    "        # Initialise the dimensions first, so the overriding functions to\n",
    "        # initialise the transition/observation/initial distributions have\n",
    "        # access to the dimensions when called from the base class\n",
    "        self._state_dim = state_dim\n",
    "        self._obs_dim = obs_dim\n",
    "\n",
    "        # Initialise the base class\n",
    "        super().__init__(tol=tol, max_iters=max_iters, verbose=verbose)\n",
    "\n",
    "\n",
    "    ######################################\n",
    "    ### HMM model parameter estimation ###\n",
    "    ######################################\n",
    "\n",
    "    # Takes in a collection of sequences of the same length, and learns\n",
    "    # the HMM model from these sequences.\n",
    "    def estimate_parameters(self, X):\n",
    "        N, T = X.shape\n",
    "        prev_log_prob = 0.0\n",
    "        curr_log_prob = np.inf\n",
    "        itr = 0\n",
    "\n",
    "        if self._verbose:\n",
    "            print(\"Estimating HMM model parameters...\")\n",
    "\n",
    "        while abs(prev_log_prob - curr_log_prob) > self._tol and itr < self._max_iters:\n",
    "            itr += 1\n",
    "\n",
    "            # E-step\n",
    "            log_gammas = []\n",
    "            log_xis = []\n",
    "            average_seq_log_likelihood = 0.0\n",
    "            for seq in X:\n",
    "                forward_lattice = self._forward(seq)\n",
    "                backward_lattice = self._backward(seq)\n",
    "                seq_log_prob = self._get_sequence_log_likelihood(forward_lattice)\n",
    "\n",
    "                seq_log_gamma = forward_lattice + backward_lattice - seq_log_prob\n",
    "                seq_log_xi = np.zeros((self._state_dim, self._state_dim, self._state_dim, T-1))\n",
    "\n",
    "                for j in range(self._state_dim):\n",
    "                    for k in range(self._state_dim):\n",
    "                        for t in range(T-1):\n",
    "                            for i in range(self._state_dim): \n",
    "                                seq_log_xi[i, j, k, t] = forward_lattice[j, t] + \\\n",
    "                                                    self._get_transition_model_log_prob(i, j, k) + \\\n",
    "                                                    self._get_emission_log_prob(k, seq[t+1]) + \\\n",
    "                                                    backward_lattice[j, t+1] - \\\n",
    "                                                    seq_log_prob\n",
    "\n",
    "                log_gammas.append(seq_log_gamma)\n",
    "                log_xis.append(seq_log_xi)\n",
    "                average_seq_log_likelihood += seq_log_prob\n",
    "\n",
    "            log_gammas = np.array(log_gammas)\n",
    "            log_xis = np.array(log_xis)\n",
    "\n",
    "            # M-step\n",
    "            self._update_transition_model(log_xis)\n",
    "            self._update_prior(log_gammas)\n",
    "            self._update_emission_model(log_gammas, X)\n",
    "\n",
    "            # Update counters\n",
    "            prev_log_prob = curr_log_prob\n",
    "            curr_log_prob = average_seq_log_likelihood / float(N)\n",
    "            if self._verbose:\n",
    "                print(f'Iter {itr}, log-likelihood loss: {curr_log_prob}, delta: {abs(prev_log_prob - curr_log_prob)}')\n",
    "\n",
    "\n",
    "    #######################################################################\n",
    "    ### Sequence likelihood estimation, forward and backward algorithms ###\n",
    "    #######################################################################\n",
    "\n",
    "    # Compute the likelihood of seeing the sequence x under \n",
    "    # the current HMM model\n",
    "    def get_sequence_likelihood(self, x):\n",
    "        lattice = self._forward(x)\n",
    "        print('forward lattice', np.exp(lattice))\n",
    "        # print('will be extracting this part only1', np.exp(self._logsumexp(lattice)))\n",
    "        # print('will be extracting this part only2', np.exp(np.apply_along_axis(self._logsumexp, 0, lattice)))\n",
    "        log_prob = self._get_sequence_log_likelihood(lattice)\n",
    "        print('forward log_prob', np.exp(log_prob))\n",
    "        return np.exp(log_prob)\n",
    "\n",
    "    # Compute the likelihood of seeing the sequence x under\n",
    "    # the current HMM model. This method uses the backward\n",
    "    # algorithm, but since forward/backward are functionally\n",
    "    # equivalent, the output should be exactly the same\n",
    "    # as get_sequence_likelihood, up to numerical rounding.\n",
    "    def get_sequence_likelihood_backward(self, x):\n",
    "        lattice = self._backward(x)\n",
    "        print('_backward lattice', np.exp(lattice))\n",
    "        # log_probs = lattice[:, 0] + self._log_phi + self._get_emission_log_prob_batch(x[0])\n",
    "        log_probs = lattice[:, :, 0] + self._log_phi + self._get_emission_log_prob_batch(x[0])\n",
    "        log_prob = self._logsumexp(log_probs)\n",
    "        return np.exp(log_prob)\n",
    "\n",
    "    def _get_sequence_log_likelihood(self, forward_lattice):\n",
    "        return self._logsumexp(forward_lattice[:, :, -1]) # P(O|lambda), probability of observing sequence given parameters\n",
    "\n",
    "    # Takes in a sequence of length T and computes the alpha values for that sequence\n",
    "    def _forward(self, x):\n",
    "        T = len(x)\n",
    "        dp_table = np.zeros((self._state_dim, self._state_dim, T))\n",
    "        for j in range(self._state_dim):\n",
    "            for k in range(self._state_dim):\n",
    "                dp_table[j, k, 0] = self._log_phi[j] + self._get_emission_log_prob(j, x[0]) \n",
    "                dp_table[j, k, 1] = self._log_phi[k] + self._get_emission_log_prob(k, x[1])\n",
    "        \n",
    "        for t in range(1, T-1): \n",
    "            for j in range(self._state_dim):\n",
    "                for k in range(self._state_dim):\n",
    "                    prev_alphas =  dp_table[j, k, t] + dp_table[j, k, t-1]\n",
    "                    single_transition_log_probs = prev_alphas + self._get_transition_model_log_prob_batch(j, k, dst=True)\n",
    "                    single_transition_log_probs += self._get_emission_log_prob(j, x[t+1])\n",
    "                    print('prev_alphas', np.exp(dp_table[j, k, t]), np.exp(dp_table[j, k, t-1]))\n",
    "                    print('prev_alphas', np.exp(prev_alphas))\n",
    "                    print('transition', np.exp(self._get_transition_model_log_prob_batch(j, k, dst=True)))\n",
    "                    print('emission', np.exp(self._get_emission_log_prob(k, x[t+1])))\n",
    "                    dp_table[j, k, t+1] = self._logsumexp(single_transition_log_probs)\n",
    "                    print('forward dp_tbl', np.exp(dp_table))\n",
    "\n",
    "        return dp_table\n",
    "\n",
    "    # Takes in a sequence of length T and computes the beta values for that sequence\n",
    "    def _backward(self, x):\n",
    "        T = len(x)\n",
    "        dp_table = np.zeros((self._state_dim, self._state_dim, T))\n",
    "\n",
    "        for t in range(T-2, 0, -1):\n",
    "            for j in range(self._state_dim): \n",
    "                for k in range(self._state_dim):\n",
    "                    prev_betas = dp_table[j, k, t+1]\n",
    "                    single_transition_log_probs =  (prev_betas \n",
    "                        + self._get_transition_model_log_prob_batch(k, j, dst=False)\n",
    "                        + self._get_emission_log_prob(k, x[t+1]))\n",
    "                    dp_table[j, k, t] = self._logsumexp(single_transition_log_probs)\n",
    "                    print('backward dp_tbl', np.exp(dp_table))\n",
    "\n",
    "        return dp_table\n",
    "\n",
    "\n",
    "    ########################\n",
    "    ### Viterbi decoding ###\n",
    "    ########################\n",
    "\n",
    "    # Uses the Viterbi algorithm to find the most likely sequence\n",
    "    # of hidden states that generates the observations. This sequence\n",
    "    # of states can be used to make predictions about future states\n",
    "    # or emissions from the Markov chain.\n",
    "    def decode(self, x):\n",
    "        T = len(x)\n",
    "        log_path_probs = np.zeros((self._state_dim, T))\n",
    "        backpointers = np.zeros((self._state_dim, T), dtype=np.int32)\n",
    "        log_path_probs[:, 0] = self._log_phi + self._get_emission_log_prob_batch(x[0])\n",
    "\n",
    "        for t in range(1, T):\n",
    "            log_probs_prev_timestep = log_path_probs[:, t-1]\n",
    "            for state in range(self._state_dim):\n",
    "                updated_probs = log_probs_prev_timestep + \\\n",
    "                                self._get_transition_model_log_prob_batch(x[t-1], state, dst=True)\n",
    "                updated_probs += self._get_emission_log_prob(state, x[t])\n",
    "                log_path_probs[state, t] = np.max(updated_probs)\n",
    "                backpointers[state, t] = np.argmax(updated_probs)\n",
    "\n",
    "        best_log_path_prob = np.max(log_path_probs[:, -1])\n",
    "        best_path_pointer = np.argmax(log_path_probs[:, -1])\n",
    "        \n",
    "        best_path = [best_path_pointer]\n",
    "        for t in range(T-1, 0, -1):\n",
    "            best_path.append(backpointers[best_path[-1], t])\n",
    "\n",
    "        return best_path[::-1], np.exp(best_log_path_prob)\n",
    "\n",
    "\n",
    "    ##################\n",
    "    ### Prediction ###\n",
    "    ##################\n",
    "\n",
    "    # Does greedy prediction of the succeeding states, solely by\n",
    "    # looking for the transition from the current state that \n",
    "    # has the highest probability, moving to the next state and\n",
    "    # selecting the emission from that state with the highest\n",
    "    # probability\n",
    "    def predict_greedy(self, x, N_future):\n",
    "        best_path, best_path_prob = self.decode(x)\n",
    "        state = best_path[-1]\n",
    "        emissions = []\n",
    "        for n in range(N_future):\n",
    "            state = np.argmax(self._get_transition_model_log_prob_batch(state, dst=False))\n",
    "            emissions.append(np.argmax(self._log_B[state, :]))\n",
    "        return emissions\n",
    "\n",
    "    # Takes a brute force combinatorial approach to predicting the\n",
    "    # succeeding states. Does this by enumerating all possible\n",
    "    # combinations of future states, concatenating them to the\n",
    "    # current observed sequence of emissions and computing the\n",
    "    # forward probability over all of them. The sequence of future\n",
    "    # states with the highest observation probability is selected\n",
    "    # as the prediction.\n",
    "    def predict_brute_force(self, x, N_future):\n",
    "        raise Exception(\"TODO: Implement this function\")\n",
    "\n",
    "\n",
    "    ################################\n",
    "    ### HMM model initialisation ###\n",
    "    ################################\n",
    "\n",
    "    def _initialise_transition_model_rand(self):\n",
    "        self._log_A = np.log([[np.random.dirichlet([1 for i in range(self._state_dim)]) for i in range(self._state_dim)] for i in range(self._state_dim)])\n",
    "        self._log_A = np.array(self._log_A)\n",
    "\n",
    "    def _initialise_prior_distribution_rand(self):\n",
    "        self._log_phi = np.log(np.random.dirichlet([1 for i in range(self._state_dim)]).flatten())\n",
    "\n",
    "    def _initialise_emission_model_rand(self):\n",
    "        self._log_B = [np.log(np.random.dirichlet([1 for i in range(self._obs_dim)])) for i in range(self._state_dim)]\n",
    "        self._log_B = np.array(self._log_B)\n",
    "\n",
    "    def _initialise_transition_model(self, transition_matrix):\n",
    "        self._log_A = np.log(transition_matrix)\n",
    "\n",
    "    def _initialise_prior_distribution(self, prior):\n",
    "        self._log_phi = np.log(prior)\n",
    "\n",
    "    def _initialise_emission_model(self, emission_matrix):\n",
    "        self._log_B = np.log(emission_matrix)\n",
    "\n",
    "    def initialise_parameters(self, transition_matrix, prior, emission_matrix):\n",
    "        assert(transition_matrix.shape == (self._state_dim, self._state_dim, self._state_dim))\n",
    "        assert(len(prior) == self._state_dim)\n",
    "        assert(emission_matrix.shape == (self._state_dim, self._obs_dim))\n",
    "\n",
    "        self._initialise_transition_model(transition_matrix)\n",
    "        self._initialise_prior_distribution(prior)\n",
    "        self._initialise_emission_model(emission_matrix)\n",
    "\n",
    "\n",
    "    #####################################\n",
    "    ### Querying HMM model parameters ###\n",
    "    #####################################\n",
    "\n",
    "    # Get transition from i --> j\n",
    "    def _get_transition_model_log_prob(self, i, j, k):\n",
    "        return self._log_A[i, j, k]\n",
    "    \n",
    "    def _get_prior_log_prob(self, state):\n",
    "        return self._log_phi[state]\n",
    "\n",
    "    def _get_emission_log_prob(self, state, obs):\n",
    "        return self._log_B[state, obs]\n",
    "\n",
    "    # Get vector of log probabilities from the transition model, corresponding\n",
    "    # to transitions of ALL states --> idx if dst is True, otherwise the\n",
    "    # transitions of idx --> ALL states if dst is False\n",
    "    # get i -> j -> k \n",
    "    def _get_transition_model_log_prob_batch(self, prev_idx, idx, dst):\n",
    "        if dst:\n",
    "            return self._log_A[:, prev_idx, idx]\n",
    "        else:\n",
    "            return self._log_A[:, idx, prev_idx]\n",
    "\n",
    "    # Get the emission probabilities of ALL states on obs\n",
    "    def _get_emission_log_prob_batch(self, obs):\n",
    "        return self._log_B[:, obs]\n",
    "\n",
    "\n",
    "    #####################################\n",
    "    ### Updating HMM model parameters ###\n",
    "    #####################################\n",
    "\n",
    "    # Updates the transition model's log probabilities.\n",
    "    def _update_transition_model(self, log_xis):\n",
    "        # log_xis is a 5-d tensor with the dimensions\n",
    "        # (num seq, num states, num states, num_states, traj len).\n",
    "        # Each value log_xis[s, i, j, k, t] represents the\n",
    "        # log transition probability of i --> j --> k at timestep t\n",
    "        # for the kth sequence. \n",
    "        #\n",
    "        # For each state-state pair, sum the gammas over all timesteps \n",
    "        # for all sequences. To transform the sum into a probability\n",
    "        # measure, normalise by summing over the destination states.\n",
    "        summed_over_timesteps = np.apply_along_axis(self._logsumexp, 4, log_xis) \n",
    "        summed_over_sequences = np.apply_along_axis(self._logsumexp, 0, summed_over_timesteps)\n",
    "        summed_over_dst_state = np.apply_along_axis(self._logsumexp, 2, summed_over_sequences)\n",
    "        self._log_A = summed_over_sequences - np.expand_dims(summed_over_dst_state, axis=0).T\n",
    "\n",
    "\n",
    "    # Updates the prior log probabilities. Effectively finding for each state\n",
    "    # the expected frequency of being in that state at the first timestep in\n",
    "    # the sequence. Computed as a log probability to avoid numerical\n",
    "    # underflow and rounding errors.\n",
    "    def _update_prior(self, log_gammas):\n",
    "        # Note that log_gammas is a 3-d tensor with dimensions\n",
    "        # (num seq, num states, traj len). We will extract the\n",
    "        # gammas for t = 0 first, then sum over all sequences\n",
    "        # for each state.\n",
    "        N = log_gammas.shape[0]\n",
    "        log_gammas_t0 = log_gammas[:, :, 0]\n",
    "        self._log_phi = np.apply_along_axis(self._logsumexp, 0, log_gammas_t0) - np.log(N)\n",
    "\n",
    "    # Updates the emission model's log probabilities.\n",
    "    def _update_emission_model(self, log_gammas, X):\n",
    "\n",
    "        # log_gammas is a 3-d tensor with the dimensions\n",
    "        # (num seq, num states, traj len). We obtain the normalising\n",
    "        # factor by summing over all data, i.e. both num seq and traj len\n",
    "        N, T = X.shape\n",
    "        summed_over_timesteps = np.apply_along_axis(self._logsumexp, 2, log_gammas)\n",
    "        summed_over_sequences = np.apply_along_axis(self._logsumexp, 0, summed_over_timesteps)\n",
    "\n",
    "        # Compute the log likelihood of the emission distribution as the\n",
    "        # expected no. of times we encounter emission vk in state i over\n",
    "        # the entire dataset\n",
    "        for state in range(self._state_dim):\n",
    "            for vk in range(self._obs_dim):\n",
    "                valid_log_probs = []\n",
    "                for n in range(N):\n",
    "                    for t in range(T):\n",
    "                        if X[n, t] == vk:\n",
    "                            valid_log_probs.append(log_gammas[n, state, t])\n",
    "\n",
    "                if len(valid_log_probs) == 0:\n",
    "                    self._log_B[state, vk] = -np.inf\n",
    "                else:\n",
    "                    self._log_B[state, vk] = self._logsumexp(np.array(valid_log_probs))\n",
    "                \n",
    "        self._log_B = self._log_B - np.expand_dims(summed_over_sequences, axis=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8c6d3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward dp_tbl [[[1.  0.7 1. ]\n",
      "  [1.  1.  1. ]]\n",
      "\n",
      " [[1.  1.  1. ]\n",
      "  [1.  1.  1. ]]]\n",
      "backward dp_tbl [[[1.   0.7  1.  ]\n",
      "  [1.   0.48 1.  ]]\n",
      "\n",
      " [[1.   1.   1.  ]\n",
      "  [1.   1.   1.  ]]]\n",
      "backward dp_tbl [[[1.   0.7  1.  ]\n",
      "  [1.   0.48 1.  ]]\n",
      "\n",
      " [[1.   0.5  1.  ]\n",
      "  [1.   1.   1.  ]]]\n",
      "backward dp_tbl [[[1.   0.7  1.  ]\n",
      "  [1.   0.48 1.  ]]\n",
      "\n",
      " [[1.   0.5  1.  ]\n",
      "  [1.   0.8  1.  ]]]\n",
      "_backward lattice [[[1.   0.7  1.  ]\n",
      "  [1.   0.48 1.  ]]\n",
      "\n",
      " [[1.   0.5  1.  ]\n",
      "  [1.   0.8  1.  ]]]\n",
      "prev_alphas 0.25 0.25\n",
      "prev_alphas 0.0625\n",
      "transition [0.9 0.5]\n",
      "emission 0.5\n",
      "forward dp_tbl [[[0.25    0.25    0.04375]\n",
      "  [0.25    0.4     1.     ]]\n",
      "\n",
      " [[0.4     0.25    1.     ]\n",
      "  [0.4     0.4     1.     ]]]\n",
      "prev_alphas 0.4 0.25\n",
      "prev_alphas 0.10000000000000002\n",
      "transition [0.1 0.5]\n",
      "emission 0.8\n",
      "forward dp_tbl [[[0.25    0.25    0.04375]\n",
      "  [0.25    0.4     0.03   ]]\n",
      "\n",
      " [[0.4     0.25    1.     ]\n",
      "  [0.4     0.4     1.     ]]]\n",
      "prev_alphas 0.25 0.4\n",
      "prev_alphas 0.10000000000000002\n",
      "transition [0.5 0.5]\n",
      "emission 0.5\n",
      "forward dp_tbl [[[0.25    0.25    0.04375]\n",
      "  [0.25    0.4     0.03   ]]\n",
      "\n",
      " [[0.4     0.25    0.08   ]\n",
      "  [0.4     0.4     1.     ]]]\n",
      "prev_alphas 0.4 0.4\n",
      "prev_alphas 0.16000000000000003\n",
      "transition [0.5 0.5]\n",
      "emission 0.8\n",
      "forward dp_tbl [[[0.25    0.25    0.04375]\n",
      "  [0.25    0.4     0.03   ]]\n",
      "\n",
      " [[0.4     0.25    0.08   ]\n",
      "  [0.4     0.4     0.128  ]]]\n",
      "forward lattice [[[0.25    0.25    0.04375]\n",
      "  [0.25    0.4     0.03   ]]\n",
      "\n",
      " [[0.4     0.25    0.08   ]\n",
      "  [0.4     0.4     0.128  ]]]\n",
      "forward log_prob 0.2817500000000001\n",
      "1.3 0.2817500000000001\n"
     ]
    }
   ],
   "source": [
    "p = 0.9\n",
    "q = 0.8\n",
    "transition_matrix = np.array([[[p, 1-p], [0.5, 0.5]], [[0.5, 0.5], [0.5, 0.5]]])\n",
    "emission_matrix = np.array([[0.5, 0.5], [q, 1-q]])\n",
    "prior = np.array([0.5, 0.5])\n",
    "\n",
    "hmm1 = HMMMultinomialSecondOrder(2, 2, verbose=True)\n",
    "hmm1.initialise_parameters(transition_matrix, prior, emission_matrix)\n",
    "test_seq1 = np.array([0, 0, 0])\n",
    "\n",
    "prob1 = hmm1.get_sequence_likelihood_backward(test_seq1)\n",
    "prob2 = hmm1.get_sequence_likelihood(test_seq1)\n",
    "\n",
    "print(prob1, prob2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4db054e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0]\n",
      "0.12487499999999999\n"
     ]
    }
   ],
   "source": [
    "p = 0.5\n",
    "q = 0.999\n",
    "transition_matrix = make_transition_matrix(p)\n",
    "emission_matrix = make_emission_matrix(q)\n",
    "prior = np.array([0.5, 0.5])\n",
    "\n",
    "hmm1.initialise_parameters(transition_matrix, prior, emission_matrix)\n",
    "test_seq2 = np.array([1, 0, 0, 1])\n",
    "path, prob = hmm1.decode(test_seq2)\n",
    "print(path)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3993b034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 1 0 1]\n",
      " [0 1 1 ... 1 0 1]\n",
      " [1 0 1 ... 1 0 1]]\n",
      "Estimating HMM model parameters...\n",
      "Iter 1, log-likelihood loss: -7.6726156109352495, delta: inf\n",
      "Iter 2, log-likelihood loss: -7.615431786708141, delta: 0.057183824227108104\n",
      "Iter 3, log-likelihood loss: -7.620815414847124, delta: 0.005383628138982743\n",
      "Iter 4, log-likelihood loss: -7.6192365151072705, delta: 0.0015788997398535898\n",
      "Iter 5, log-likelihood loss: -7.612326174715188, delta: 0.0069103403920829365\n",
      "Iter 6, log-likelihood loss: -7.5985124039371845, delta: 0.013813770778003054\n",
      "Iter 7, log-likelihood loss: -7.581638692056695, delta: 0.01687371188048914\n",
      "Iter 8, log-likelihood loss: -7.56698867217573, delta: 0.014650019880965814\n",
      "Iter 9, log-likelihood loss: -7.558699536572356, delta: 0.00828913560337341\n",
      "Iter 10, log-likelihood loss: -7.558593810143422, delta: 0.00010572642893436068\n",
      "Iter 11, log-likelihood loss: -7.565966338528428, delta: 0.007372528385006127\n",
      "Iter 12, log-likelihood loss: -7.578184559947095, delta: 0.012218221418667241\n",
      "Iter 13, log-likelihood loss: -7.5918749175531985, delta: 0.013690357606103376\n",
      "Iter 14, log-likelihood loss: -7.60416066496233, delta: 0.012285747409131353\n",
      "Iter 15, log-likelihood loss: -7.613399314825807, delta: 0.009238649863476667\n",
      "Iter 16, log-likelihood loss: -7.619245091920836, delta: 0.005845777095029092\n",
      "Iter 17, log-likelihood loss: -7.622255566007676, delta: 0.003010474086840631\n",
      "Iter 18, log-likelihood loss: -7.623372167116026, delta: 0.0011166011083494354\n",
      "Iter 19, log-likelihood loss: -7.623494665822086, delta: 0.0001224987060606253\n",
      "Iter 20, log-likelihood loss: -7.623231357515536, delta: 0.0002633083065504138\n",
      "Iter 21, log-likelihood loss: -7.622826478923282, delta: 0.00040487859225368794\n",
      "Iter 22, log-likelihood loss: -7.622230034209707, delta: 0.0005964447135751172\n",
      "Iter 23, log-likelihood loss: -7.621252724390221, delta: 0.000977309819486294\n",
      "Iter 24, log-likelihood loss: -7.619735178063778, delta: 0.0015175463264425204\n",
      "Iter 25, log-likelihood loss: -7.617667254760338, delta: 0.002067923303440189\n",
      "Iter 26, log-likelihood loss: -7.6152229152330015, delta: 0.002444339527336581\n",
      "Iter 27, log-likelihood loss: -7.6127150029434, delta: 0.002507912289601677\n",
      "Iter 28, log-likelihood loss: -7.610502753748143, delta: 0.0022122491952565326\n",
      "Iter 29, log-likelihood loss: -7.608893283062555, delta: 0.0016094706855884766\n",
      "Iter 30, log-likelihood loss: -7.608069910152561, delta: 0.0008233729099940135\n",
      "Iter 31, log-likelihood loss: -7.608063806264279, delta: 6.103888281927539e-06\n",
      "Transition\n",
      "[[[0.5001843  0.4998157 ]\n",
      "  [0.49437887 0.49394322]]\n",
      "\n",
      " [[0.50540084 0.50641506]\n",
      "  [0.49948234 0.50051766]]]\n",
      "Emission\n",
      "[[0.43811679 0.56188321]\n",
      " [0.58023233 0.41976767]]\n",
      "Prior\n",
      "[0.46819444 0.54153211]\n"
     ]
    }
   ],
   "source": [
    "p = 0.5\n",
    "q = 0.5\n",
    "transition_matrix = make_transition_matrix(p)\n",
    "emission_matrix = make_emission_matrix(q)\n",
    "prior = np.array([0.5, 0.5])\n",
    "    \n",
    "data = np.array([draw_sequence(transition_matrix, emission_matrix, prior, 12) for i in range(500)])\n",
    "print(data)\n",
    "\n",
    "hmm1 = HMMMultinomialSecondOrder(2, 2, verbose=True)\n",
    "hmm1._initialise_transition_model(transition_matrix)\n",
    "hmm1._initialise_prior_distribution(prior)\n",
    "hmm1.estimate_parameters(data)\n",
    "\n",
    "print(\"Transition\")\n",
    "print(np.exp(hmm1._log_A))\n",
    "print(\"Emission\")\n",
    "print(np.exp(hmm1._log_B))\n",
    "print(\"Prior\")\n",
    "print(np.exp(hmm1._log_phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eca6f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
