{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ff0155",
   "metadata": {},
   "source": [
    "# Tests and sanity checks of HMM implementations\n",
    "\n",
    "For basic functionality tests of our HMM implementations, we use a simple Markov chain setup based on a coin-flipping task.\n",
    "\n",
    "Consider a coin-flipping task where there are 2 states: 1) we are holding an unbiased coin and 2) we are holding a biased coin. Between coin flips, there is a probability p that we switch coins. This gives a Markov chain with a 2x2 transition matrix. The unbiased coin state has index 0 and the biased coin state has index 1.\n",
    "    \n",
    "There are two possible observations/emissions from each coin flip. We can either observe: 1) heads or 2) tails. The probability of observing heads/tails from the unbiased coin state is 0.5, and the probability from the biased coin state is q/1-q, for some probability q. The heads emission has index 0 and the tails emission has index 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "addb039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49166a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transition_matrix(p):\n",
    "    return np.array([[p, 1-p], [1-p, p]])\n",
    "\n",
    "def make_emission_matrix(q):\n",
    "    return np.array([[0.5, 0.5], [q, 1-q]])\n",
    "\n",
    "def draw_sequence(A, B, prior, T):\n",
    "    num_states, num_obs = B.shape\n",
    "    state = np.random.choice(num_states, 1, p=prior).item()\n",
    "    emissions = [np.random.choice(num_obs, 1, p=B[state].flatten()).item()]\n",
    "    for t in range(1, T):\n",
    "        state = np.random.choice(num_states, 1, p=A[state].flatten()).item()\n",
    "        emissions.append(np.random.choice(num_obs, 1, p=B[state].flatten()).item())\n",
    "    return emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd8802",
   "metadata": {},
   "source": [
    "## Multinomial HMM based on 1st order Markov chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bb1e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertainty_motion_prediction.predictor import HMMMultinomialFirstOrder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6205abb0",
   "metadata": {},
   "source": [
    "We test the forward/backward algorithms used for estimating the likelihood of a given sequence of emissions/observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d901f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "q = 0.5\n",
    "transition_matrix = make_transition_matrix(p)\n",
    "emission_matrix = make_emission_matrix(q)\n",
    "prior = np.array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0424ba74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0625 0.06250000000000003\n"
     ]
    }
   ],
   "source": [
    "hmm1 = HMMMultinomialFirstOrder(2, 2, verbose=True)\n",
    "hmm1.initialise_parameters(transition_matrix, prior, emission_matrix)\n",
    "test_seq1 = np.array([0, 0, 1, 0])\n",
    "prob1 = hmm1.get_sequence_likelihood_backward(test_seq1)\n",
    "prob2 = hmm1.get_sequence_likelihood(test_seq1)\n",
    "\n",
    "print(prob1, prob2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c954140",
   "metadata": {},
   "source": [
    "We test the decoding algorithms used to estimate the most likely sequence of hidden states that produced the sequence of emissions/observations in our data. The decoding algorithm implemented is the Viterbi algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80ade997",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "q = 0.999 # Use 0.999 instead of 1, because I have yet to handle the case of np.log(0) in log-sum-exp function.\n",
    "transition_matrix = make_transition_matrix(p)\n",
    "emission_matrix = make_emission_matrix(q)\n",
    "prior = np.array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caaf85a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1]\n",
      "0.06237506250000003\n"
     ]
    }
   ],
   "source": [
    "hmm2 = HMMMultinomialFirstOrder(2, 2, verbose=True)\n",
    "hmm2.initialise_parameters(transition_matrix, prior, emission_matrix)\n",
    "test_seq2 = np.array([1, 0, 0])\n",
    "path, prob = hmm2.decode(test_seq2)\n",
    "print(path)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b462d",
   "metadata": {},
   "source": [
    "Test the estimation of HMM model parameters from data. This uses the specialised form of the EM algorithm, the Baum-Welch algorithm. The algorithm can be prone to local minima, and may not converge well when all the parameters to be estimated are not well-initialised. You can test its robustness to initialisation but selectively initialising some of the parameters using the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73edc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "q = 0.999 # Use 0.999 instead of 1, because I have yet to handle the case of np.log(0) in log-sum-exp function.\n",
    "transition_matrix = make_transition_matrix(p)\n",
    "emission_matrix = make_emission_matrix(q)\n",
    "prior = np.array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f45899fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 1 1 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 1]]\n",
      "Estimating HMM model parameters...\n",
      "Iter 1, log-likelihood loss: -7.024722607974975, delta: inf\n",
      "Iter 2, log-likelihood loss: -5.664936102659883, delta: 1.3597865053150917\n",
      "Iter 3, log-likelihood loss: -5.66131013548803, delta: 0.0036259671718532616\n",
      "Iter 4, log-likelihood loss: -5.658889304612112, delta: 0.0024208308759181563\n",
      "Iter 5, log-likelihood loss: -5.657255492287608, delta: 0.0016338123245036584\n",
      "Iter 6, log-likelihood loss: -5.656145098736711, delta: 0.0011103935508973706\n",
      "Iter 7, log-likelihood loss: -5.655387159901618, delta: 0.0007579388350924532\n",
      "Iter 8, log-likelihood loss: -5.65486849272569, delta: 0.0005186671759283001\n",
      "Iter 9, log-likelihood loss: -5.654513089781826, delta: 0.0003554029438639361\n",
      "Iter 10, log-likelihood loss: -5.654269421616268, delta: 0.00024366816555776438\n",
      "Iter 11, log-likelihood loss: -5.654102342983989, delta: 0.00016707863227960473\n",
      "Iter 12, log-likelihood loss: -5.653987797731999, delta: 0.00011454525198928422\n",
      "Iter 13, log-likelihood loss: -5.653909288519687, delta: 7.85092123125608e-05\n",
      "Iter 14, log-likelihood loss: -5.653855492412965, delta: 5.379610672218149e-05\n",
      "Iter 15, log-likelihood loss: -5.65381863681706, delta: 3.685559590493881e-05\n",
      "Iter 16, log-likelihood loss: -5.653793387648064, delta: 2.5249168995600257e-05\n",
      "Iter 17, log-likelihood loss: -5.653776086052482, delta: 1.730159558199773e-05\n",
      "Iter 18, log-likelihood loss: -5.6537642237406756, delta: 1.1862311806609682e-05\n",
      "Iter 19, log-likelihood loss: -5.653756082226236, delta: 8.141514439508057e-06\n",
      "Transition\n",
      "[[0.31238621 0.68761379]\n",
      " [0.17969314 0.82030686]]\n",
      "Emission\n",
      "[[0.75423715 0.24576285]\n",
      " [0.74447049 0.25552951]]\n",
      "Prior\n",
      "[0.83414374 0.16585626]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([draw_sequence(transition_matrix, emission_matrix, prior, 10) for i in range(500)])\n",
    "print(data)\n",
    "\n",
    "# Don't give the correct values for any of the parameters. Use random initialisation.\n",
    "hmm3 = HMMMultinomialFirstOrder(2, 2, verbose=True)\n",
    "hmm3.estimate_parameters(data)\n",
    "\n",
    "print(\"Transition\")\n",
    "print(np.exp(hmm3._log_A))\n",
    "print(\"Emission\")\n",
    "print(np.exp(hmm3._log_B))\n",
    "print(\"Prior\")\n",
    "print(np.exp(hmm3._log_phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15669531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 1 1 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 1]]\n",
      "Estimating HMM model parameters...\n",
      "Iter 1, log-likelihood loss: -7.3160232371448695, delta: inf\n",
      "Iter 2, log-likelihood loss: -5.653891440391395, delta: 1.6621317967534743\n",
      "Iter 3, log-likelihood loss: -5.653879230677302, delta: 1.220971409310323e-05\n",
      "Iter 4, log-likelihood loss: -5.653867394295645, delta: 1.1836381657026607e-05\n",
      "Iter 5, log-likelihood loss: -5.653855915128039, delta: 1.1479167605799034e-05\n",
      "Iter 6, log-likelihood loss: -5.653844777992499, delta: 1.113713554001805e-05\n",
      "Iter 7, log-likelihood loss: -5.653833968573029, delta: 1.0809419470625414e-05\n",
      "Iter 8, log-likelihood loss: -5.653823473355083, delta: 1.0495217945383217e-05\n",
      "Iter 9, log-likelihood loss: -5.65381327956661, delta: 1.0193788472889764e-05\n",
      "Iter 10, log-likelihood loss: -5.653803375123801, delta: 9.9044428090167e-06\n",
      "Transition\n",
      "[[0.45704322 0.54295678]\n",
      " [0.45960241 0.54039759]]\n",
      "Emission\n",
      "[[0.68387724 0.31612276]\n",
      " [0.80043714 0.19956286]]\n",
      "Prior\n",
      "[0.44145927 0.55854073]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([draw_sequence(transition_matrix, emission_matrix, prior, 10) for i in range(500)])\n",
    "print(data)\n",
    "\n",
    "# Initialise the transition and prior models using correct values, and try to estimate the emission probabilities.\n",
    "hmm3 = HMMMultinomialFirstOrder(2, 2, verbose=True)\n",
    "hmm3._initialise_transition_model(transition_matrix)\n",
    "hmm3._initialise_prior_distribution(prior)\n",
    "hmm3.estimate_parameters(data)\n",
    "\n",
    "print(\"Transition\")\n",
    "print(np.exp(hmm3._log_A))\n",
    "print(\"Emission\")\n",
    "print(np.exp(hmm3._log_B))\n",
    "print(\"Prior\")\n",
    "print(np.exp(hmm3._log_phi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
